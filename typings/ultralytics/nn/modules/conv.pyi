"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn

"""Convolution modules."""
__all__ = ("Conv", "Conv2", "LightConv", "DWConv", "DWConvTranspose2d", "ConvTranspose", "Focus", "GhostConv", "ChannelAttention", "SpatialAttention", "CBAM", "Concat", "RepConv", "Index")
def autopad(k, p=..., d=...): # -> int | list[Any]:
    """Pad to 'same' shape outputs."""
    ...

class Conv(nn.Module):
    """Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)."""
    default_act = ...
    def __init__(self, c1, c2, k=..., s=..., p=..., g=..., d=..., act=...) -> None:
        """Initialize Conv layer with given arguments including activation."""
        ...
    
    def forward(self, x): # -> Any:
        """Apply convolution, batch normalization and activation to input tensor."""
        ...
    
    def forward_fuse(self, x): # -> Any:
        """Apply convolution and activation without batch normalization."""
        ...
    


class Conv2(Conv):
    """Simplified RepConv module with Conv fusing."""
    def __init__(self, c1, c2, k=..., s=..., p=..., g=..., d=..., act=...) -> None:
        """Initialize Conv layer with given arguments including activation."""
        ...
    
    def forward(self, x): # -> Any:
        """Apply convolution, batch normalization and activation to input tensor."""
        ...
    
    def forward_fuse(self, x): # -> Any:
        """Apply fused convolution, batch normalization and activation to input tensor."""
        ...
    
    def fuse_convs(self): # -> None:
        """Fuse parallel convolutions."""
        ...
    


class LightConv(nn.Module):
    """
    Light convolution with args(ch_in, ch_out, kernel).

    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/backbones/hgnet_v2.py
    """
    def __init__(self, c1, c2, k=..., act=...) -> None:
        """Initialize Conv layer with given arguments including activation."""
        ...
    
    def forward(self, x): # -> Any:
        """Apply 2 convolutions to input tensor."""
        ...
    


class DWConv(Conv):
    """Depth-wise convolution."""
    def __init__(self, c1, c2, k=..., s=..., d=..., act=...) -> None:
        """Initialize Depth-wise convolution with given parameters."""
        ...
    


class DWConvTranspose2d(nn.ConvTranspose2d):
    """Depth-wise transpose convolution."""
    def __init__(self, c1, c2, k=..., s=..., p1=..., p2=...) -> None:
        """Initialize DWConvTranspose2d class with given parameters."""
        ...
    


class ConvTranspose(nn.Module):
    """Convolution transpose 2d layer."""
    default_act = ...
    def __init__(self, c1, c2, k=..., s=..., p=..., bn=..., act=...) -> None:
        """Initialize ConvTranspose2d layer with batch normalization and activation function."""
        ...
    
    def forward(self, x): # -> Any:
        """Applies transposed convolutions, batch normalization and activation to input."""
        ...
    
    def forward_fuse(self, x): # -> Any:
        """Applies activation and convolution transpose operation to input."""
        ...
    


class Focus(nn.Module):
    """Focus wh information into c-space."""
    def __init__(self, c1, c2, k=..., s=..., p=..., g=..., act=...) -> None:
        """Initializes Focus object with user defined channel, convolution, padding, group and activation values."""
        ...
    
    def forward(self, x): # -> Any:
        """
        Applies convolution to concatenated tensor and returns the output.

        Input shape is (b,c,w,h) and output shape is (b,4c,w/2,h/2).
        """
        ...
    


class GhostConv(nn.Module):
    """Ghost Convolution https://github.com/huawei-noah/ghostnet."""
    def __init__(self, c1, c2, k=..., s=..., g=..., act=...) -> None:
        """Initializes Ghost Convolution module with primary and cheap operations for efficient feature learning."""
        ...
    
    def forward(self, x): # -> Tensor:
        """Forward propagation through a Ghost Bottleneck layer with skip connection."""
        ...
    


class RepConv(nn.Module):
    """
    RepConv is a basic rep-style block, including training and deploy status.

    This module is used in RT-DETR.
    Based on https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py
    """
    default_act = ...
    def __init__(self, c1, c2, k=..., s=..., p=..., g=..., d=..., act=..., bn=..., deploy=...) -> None:
        """Initializes Light Convolution layer with inputs, outputs & optional activation function."""
        ...
    
    def forward_fuse(self, x): # -> Any:
        """Forward process."""
        ...
    
    def forward(self, x): # -> Any:
        """Forward process."""
        ...
    
    def get_equivalent_kernel_bias(self): # -> tuple[Tensor | Any | int, Tensor | Any | int]:
        """Returns equivalent kernel and bias by adding 3x3 kernel, 1x1 kernel and identity kernel with their biases."""
        ...
    
    def fuse_convs(self): # -> None:
        """Combines two convolution layers into a single layer and removes unused attributes from the class."""
        ...
    


class ChannelAttention(nn.Module):
    """Channel-attention module https://github.com/open-mmlab/mmdetection/tree/v3.0.0rc1/configs/rtmdet."""
    def __init__(self, channels: int) -> None:
        """Initializes the class and sets the basic configurations and instance variables required."""
        ...
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Applies forward pass using activation on convolutions of the input, optionally using batch normalization."""
        ...
    


class SpatialAttention(nn.Module):
    """Spatial-attention module."""
    def __init__(self, kernel_size=...) -> None:
        """Initialize Spatial-attention module with kernel size argument."""
        ...
    
    def forward(self, x):
        """Apply channel and spatial attention on input for feature recalibration."""
        ...
    


class CBAM(nn.Module):
    """Convolutional Block Attention Module."""
    def __init__(self, c1, kernel_size=...) -> None:
        """Initialize CBAM with given input channel (c1) and kernel size."""
        ...
    
    def forward(self, x): # -> Any:
        """Applies the forward pass through C1 module."""
        ...
    


class Concat(nn.Module):
    """Concatenate a list of tensors along dimension."""
    def __init__(self, dimension=...) -> None:
        """Concatenates a list of tensors along a specified dimension."""
        ...
    
    def forward(self, x): # -> Tensor:
        """Forward pass for the YOLOv8 mask Proto module."""
        ...
    


class Index(nn.Module):
    """Returns a particular index of the input."""
    def __init__(self, index=...) -> None:
        """Returns a particular index of the input."""
        ...
    
    def forward(self, x):
        """
        Forward pass.

        Expects a list of tensors as input.
        """
        ...
    



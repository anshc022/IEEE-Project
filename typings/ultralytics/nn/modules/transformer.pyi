"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn

"""Transformer modules."""
__all__ = ("TransformerEncoderLayer", "TransformerLayer", "TransformerBlock", "MLPBlock", "LayerNorm2d", "AIFI", "DeformableTransformerDecoder", "DeformableTransformerDecoderLayer", "MSDeformAttn", "MLP")
class TransformerEncoderLayer(nn.Module):
    """Defines a single layer of the transformer encoder."""
    def __init__(self, c1, cm=..., num_heads=..., dropout=..., act=..., normalize_before=...) -> None:
        """Initialize the TransformerEncoderLayer with specified parameters."""
        ...
    
    @staticmethod
    def with_pos_embed(tensor, pos=...):
        """Add position embeddings to the tensor if provided."""
        ...
    
    def forward_post(self, src, src_mask=..., src_key_padding_mask=..., pos=...): # -> Any:
        """Performs forward pass with post-normalization."""
        ...
    
    def forward_pre(self, src, src_mask=..., src_key_padding_mask=..., pos=...):
        """Performs forward pass with pre-normalization."""
        ...
    
    def forward(self, src, src_mask=..., src_key_padding_mask=..., pos=...): # -> Any:
        """Forward propagates the input through the encoder module."""
        ...
    


class AIFI(TransformerEncoderLayer):
    """Defines the AIFI transformer layer."""
    def __init__(self, c1, cm=..., num_heads=..., dropout=..., act=..., normalize_before=...) -> None:
        """Initialize the AIFI instance with specified parameters."""
        ...
    
    def forward(self, x): # -> Any:
        """Forward pass for the AIFI transformer layer."""
        ...
    
    @staticmethod
    def build_2d_sincos_position_embedding(w, h, embed_dim=..., temperature=...): # -> Tensor:
        """Builds 2D sine-cosine position embedding."""
        ...
    


class TransformerLayer(nn.Module):
    """Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance)."""
    def __init__(self, c, num_heads) -> None:
        """Initializes a self-attention mechanism using linear transformations and multi-head attention."""
        ...
    
    def forward(self, x):
        """Apply a transformer block to the input x and return the output."""
        ...
    


class TransformerBlock(nn.Module):
    """Vision Transformer https://arxiv.org/abs/2010.11929."""
    def __init__(self, c1, c2, num_heads, num_layers) -> None:
        """Initialize a Transformer module with position embedding and specified number of heads and layers."""
        ...
    
    def forward(self, x): # -> Any:
        """Forward propagates the input through the bottleneck module."""
        ...
    


class MLPBlock(nn.Module):
    """Implements a single block of a multi-layer perceptron."""
    def __init__(self, embedding_dim, mlp_dim, act=...) -> None:
        """Initialize the MLPBlock with specified embedding dimension, MLP dimension, and activation function."""
        ...
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass for the MLPBlock."""
        ...
    


class MLP(nn.Module):
    """Implements a simple multi-layer perceptron (also called FFN)."""
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, act=..., sigmoid=...) -> None:
        """Initialize the MLP with specified input, hidden, output dimensions and number of layers."""
        ...
    
    def forward(self, x): # -> Any:
        """Forward pass for the entire MLP."""
        ...
    


class LayerNorm2d(nn.Module):
    """
    2D Layer Normalization module inspired by Detectron2 and ConvNeXt implementations.

    Original implementations in
    https://github.com/facebookresearch/detectron2/blob/main/detectron2/layers/batch_norm.py
    and
    https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py.
    """
    def __init__(self, num_channels, eps=...) -> None:
        """Initialize LayerNorm2d with the given parameters."""
        ...
    
    def forward(self, x):
        """Perform forward pass for 2D layer normalization."""
        ...
    


class MSDeformAttn(nn.Module):
    """
    Multiscale Deformable Attention Module based on Deformable-DETR and PaddleDetection implementations.

    https://github.com/fundamentalvision/Deformable-DETR/blob/main/models/ops/modules/ms_deform_attn.py
    """
    def __init__(self, d_model=..., n_levels=..., n_heads=..., n_points=...) -> None:
        """Initialize MSDeformAttn with the given parameters."""
        ...
    
    def forward(self, query, refer_bbox, value, value_shapes, value_mask=...): # -> Any:
        """
        Perform forward pass for multiscale deformable attention.

        https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/transformers/deformable_transformer.py

        Args:
            query (torch.Tensor): [bs, query_length, C]
            refer_bbox (torch.Tensor): [bs, query_length, n_levels, 2], range in [0, 1], top-left (0,0),
                bottom-right (1, 1), including padding area
            value (torch.Tensor): [bs, value_length, C]
            value_shapes (List): [n_levels, 2], [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]
            value_mask (Tensor): [bs, value_length], True for non-padding elements, False for padding elements

        Returns:
            output (Tensor): [bs, Length_{query}, C]
        """
        ...
    


class DeformableTransformerDecoderLayer(nn.Module):
    """
    Deformable Transformer Decoder Layer inspired by PaddleDetection and Deformable-DETR implementations.

    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/transformers/deformable_transformer.py
    https://github.com/fundamentalvision/Deformable-DETR/blob/main/models/deformable_transformer.py
    """
    def __init__(self, d_model=..., n_heads=..., d_ffn=..., dropout=..., act=..., n_levels=..., n_points=...) -> None:
        """Initialize the DeformableTransformerDecoderLayer with the given parameters."""
        ...
    
    @staticmethod
    def with_pos_embed(tensor, pos):
        """Add positional embeddings to the input tensor, if provided."""
        ...
    
    def forward_ffn(self, tgt): # -> Any:
        """Perform forward pass through the Feed-Forward Network part of the layer."""
        ...
    
    def forward(self, embed, refer_bbox, feats, shapes, padding_mask=..., attn_mask=..., query_pos=...): # -> Any:
        """Perform the forward pass through the entire decoder layer."""
        ...
    


class DeformableTransformerDecoder(nn.Module):
    """
    Implementation of Deformable Transformer Decoder based on PaddleDetection.

    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/transformers/deformable_transformer.py
    """
    def __init__(self, hidden_dim, decoder_layer, num_layers, eval_idx=...) -> None:
        """Initialize the DeformableTransformerDecoder with the given parameters."""
        ...
    
    def forward(self, embed, refer_bbox, feats, shapes, bbox_head, score_head, pos_mlp, attn_mask=..., padding_mask=...): # -> tuple[Tensor, Tensor]:
        """Perform the forward pass through the entire decoder."""
        ...
    



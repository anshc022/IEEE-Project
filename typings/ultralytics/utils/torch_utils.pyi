"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from contextlib import contextmanager
from pathlib import Path
from typing import Union
from ultralytics.utils import WINDOWS, __version__
from ultralytics.utils.checks import check_version

TORCH_1_9 = ...
TORCH_1_13 = ...
TORCH_2_0 = ...
TORCH_2_4 = ...
TORCHVISION_0_10 = ...
TORCHVISION_0_11 = ...
TORCHVISION_0_13 = ...
TORCHVISION_0_18 = ...
if WINDOWS and check_version(torch.__version__, "==2.4.0"):
    ...
@contextmanager
def torch_distributed_zero_first(local_rank: int): # -> Generator[None, Any, None]:
    """Ensures all processes in distributed training wait for the local master (rank 0) to complete a task first."""
    ...

def smart_inference_mode(): # -> Callable[..., Any]:
    """Applies torch.inference_mode() decorator if torch>=1.9.0 else torch.no_grad() decorator."""
    ...

def autocast(enabled: bool, device: str = ...): # -> torch.amp.autocast_mode.autocast | torch.cuda.amp.autocast_mode.autocast:
    """
    Get the appropriate autocast context manager based on PyTorch version and AMP setting.

    This function returns a context manager for automatic mixed precision (AMP) training that is compatible with both
    older and newer versions of PyTorch. It handles the differences in the autocast API between PyTorch versions.

    Args:
        enabled (bool): Whether to enable automatic mixed precision.
        device (str, optional): The device to use for autocast. Defaults to 'cuda'.

    Returns:
        (torch.amp.autocast): The appropriate autocast context manager.

    Note:
        - For PyTorch versions 1.13 and newer, it uses `torch.amp.autocast`.
        - For older versions, it uses `torch.cuda.autocast`.

    Example:
        ```python
        with autocast(amp=True):
            # Your mixed precision operations here
            pass
        ```
    """
    ...

def get_cpu_info():
    """Return a string with system CPU information, i.e. 'Apple M2'."""
    ...

def get_gpu_info(index): # -> str:
    """Return a string with system GPU information, i.e. 'Tesla T4, 15102MiB'."""
    ...

def select_device(device=..., batch=..., newline=..., verbose=...): # -> <subclass of str and device> | str | device:
    """
    Selects the appropriate PyTorch device based on the provided arguments.

    The function takes a string specifying the device or a torch.device object and returns a torch.device object
    representing the selected device. The function also validates the number of available devices and raises an
    exception if the requested device(s) are not available.

    Args:
        device (str | torch.device, optional): Device string or torch.device object.
            Options are 'None', 'cpu', or 'cuda', or '0' or '0,1,2,3'. Defaults to an empty string, which auto-selects
            the first available GPU, or CPU if no GPU is available.
        batch (int, optional): Batch size being used in your model. Defaults to 0.
        newline (bool, optional): If True, adds a newline at the end of the log string. Defaults to False.
        verbose (bool, optional): If True, logs the device information. Defaults to True.

    Returns:
        (torch.device): Selected device.

    Raises:
        ValueError: If the specified device is not available or if the batch size is not a multiple of the number of
            devices when using multiple GPUs.

    Examples:
        >>> select_device("cuda:0")
        device(type='cuda', index=0)

        >>> select_device("cpu")
        device(type='cpu')

    Note:
        Sets the 'CUDA_VISIBLE_DEVICES' environment variable for specifying which GPUs to use.
    """
    ...

def time_sync(): # -> float:
    """PyTorch-accurate time."""
    ...

def fuse_conv_and_bn(conv, bn): # -> Conv2d:
    """Fuse Conv2d() and BatchNorm2d() layers https://tehnokv.com/posts/fusing-batchnorm-and-conv/."""
    ...

def fuse_deconv_and_bn(deconv, bn): # -> ConvTranspose2d:
    """Fuse ConvTranspose2d() and BatchNorm2d() layers."""
    ...

def model_info(model, detailed=..., verbose=..., imgsz=...): # -> tuple[int, int, int, float | Any] | None:
    """Print and return detailed model information layer by layer."""
    ...

def get_num_params(model): # -> int:
    """Return the total number of parameters in a YOLO model."""
    ...

def get_num_gradients(model): # -> int:
    """Return the total number of parameters with gradients in a YOLO model."""
    ...

def model_info_for_loggers(trainer): # -> dict[str, Any]:
    """
    Return model info dict with useful model information.

    Example:
        YOLOv8n info for loggers
        ```python
        results = {
            "model/parameters": 3151904,
            "model/GFLOPs": 8.746,
            "model/speed_ONNX(ms)": 41.244,
            "model/speed_TensorRT(ms)": 3.211,
            "model/speed_PyTorch(ms)": 18.755,
        }
        ```
    """
    ...

def get_flops(model, imgsz=...): # -> float:
    """Return a YOLO model's FLOPs."""
    ...

def get_flops_with_torch_profiler(model, imgsz=...): # -> float:
    """Compute model FLOPs (thop package alternative, but 2-10x slower unfortunately)."""
    ...

def initialize_weights(model): # -> None:
    """Initialize model weights to random values."""
    ...

def scale_img(img, ratio=..., same_shape=..., gs=...): # -> Tensor:
    """Scales and pads an image tensor, optionally maintaining aspect ratio and padding to gs multiple."""
    ...

def copy_attr(a, b, include=..., exclude=...): # -> None:
    """Copies attributes from object 'b' to object 'a', with options to include/exclude certain attributes."""
    ...

def get_latest_opset(): # -> int:
    """Return the second-most recent ONNX opset version supported by this version of PyTorch, adjusted for maturity."""
    ...

def intersect_dicts(da, db, exclude=...): # -> dict[Any, Any]:
    """Returns a dictionary of intersecting keys with matching shapes, excluding 'exclude' keys, using da values."""
    ...

def is_parallel(model): # -> bool:
    """Returns True if model is of type DP or DDP."""
    ...

def de_parallel(model):
    """De-parallelize a model: returns single-GPU model if model is of type DP or DDP."""
    ...

def one_cycle(y1=..., y2=..., steps=...): # -> Callable[..., float]:
    """Returns a lambda function for sinusoidal ramp from y1 to y2 https://arxiv.org/pdf/1812.01187.pdf."""
    ...

def init_seeds(seed=..., deterministic=...): # -> None:
    """Initialize random number generator (RNG) seeds https://pytorch.org/docs/stable/notes/randomness.html."""
    ...

def unset_deterministic(): # -> None:
    """Unsets all the configurations applied for deterministic training."""
    ...

class ModelEMA:
    """
    Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models. Keeps a moving
    average of everything in the model state_dict (parameters and buffers).

    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage

    To disable EMA set the `enabled` attribute to `False`.
    """
    def __init__(self, model, decay=..., tau=..., updates=...) -> None:
        """Initialize EMA for 'model' with given arguments."""
        ...
    
    def update(self, model): # -> None:
        """Update EMA parameters."""
        ...
    
    def update_attr(self, model, include=..., exclude=...): # -> None:
        """Updates attributes and saves stripped model with optimizer removed."""
        ...
    


def strip_optimizer(f: Union[str, Path] = ..., s: str = ..., updates: dict = ...) -> dict:
    """
    Strip optimizer from 'f' to finalize training, optionally save as 's'.

    Args:
        f (str): file path to model to strip the optimizer from. Default is 'best.pt'.
        s (str): file path to save the model with stripped optimizer to. If not provided, 'f' will be overwritten.
        updates (dict): a dictionary of updates to overlay onto the checkpoint before saving.

    Returns:
        (dict): The combined checkpoint dictionary.

    Example:
        ```python
        from pathlib import Path
        from ultralytics.utils.torch_utils import strip_optimizer

        for f in Path("path/to/model/checkpoints").rglob("*.pt"):
            strip_optimizer(f)
        ```

    Note:
        Use `ultralytics.nn.torch_safe_load` for missing modules with `x = torch_safe_load(f)[0]`
    """
    ...

def convert_optimizer_state_dict_to_fp16(state_dict):
    """
    Converts the state_dict of a given optimizer to FP16, focusing on the 'state' key for tensor conversions.

    This method aims to reduce storage size without altering 'param_groups' as they contain non-tensor data.
    """
    ...

@contextmanager
def cuda_memory_usage(device=...): # -> Generator[dict[str, int], Any, None]:
    """
    Monitor and manage CUDA memory usage.

    This function checks if CUDA is available and, if so, empties the CUDA cache to free up unused memory.
    It then yields a dictionary containing memory usage information, which can be updated by the caller.
    Finally, it updates the dictionary with the amount of memory reserved by CUDA on the specified device.

    Args:
        device (torch.device, optional): The CUDA device to query memory usage for. Defaults to None.

    Yields:
        (dict): A dictionary with a key 'memory' initialized to 0, which will be updated with the reserved memory.
    """
    ...

def profile(input, ops, n=..., device=..., max_num_obj=...):
    """
    Ultralytics speed, memory and FLOPs profiler.

    Example:
        ```python
        from ultralytics.utils.torch_utils import profile

        input = torch.randn(16, 3, 640, 640)
        m1 = lambda x: x * torch.sigmoid(x)
        m2 = nn.SiLU()
        profile(input, [m1, m2], n=100)  # profile over 100 iterations
        ```
    """
    ...

class EarlyStopping:
    """Early stopping class that stops training when a specified number of epochs have passed without improvement."""
    def __init__(self, patience=...) -> None:
        """
        Initialize early stopping object.

        Args:
            patience (int, optional): Number of epochs to wait after fitness stops improving before stopping.
        """
        ...
    
    def __call__(self, epoch, fitness): # -> Literal[False]:
        """
        Check whether to stop training.

        Args:
            epoch (int): Current epoch of training
            fitness (float): Fitness value of current epoch

        Returns:
            (bool): True if training should stop, False otherwise
        """
        ...
    


class FXModel(nn.Module):
    """
    A custom model class for torch.fx compatibility.

    This class extends `torch.nn.Module` and is designed to ensure compatibility with torch.fx for tracing and graph manipulation.
    It copies attributes from an existing model and explicitly sets the model attribute to ensure proper copying.

    Args:
        model (torch.nn.Module): The original model to wrap for torch.fx compatibility.
    """
    def __init__(self, model) -> None:
        """
        Initialize the FXModel.

        Args:
            model (torch.nn.Module): The original model to wrap for torch.fx compatibility.
        """
        ...
    
    def forward(self, x):
        """
        Forward pass through the model.

        This method performs the forward pass through the model, handling the dependencies between layers and saving intermediate outputs.

        Args:
            x (torch.Tensor): The input tensor to the model.

        Returns:
            (torch.Tensor): The output tensor from the model.
        """
        ...
    


